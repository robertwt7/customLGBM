import subprocess
import math
import numpy as np
import pandas as pd 
from pathlib import Path
import lightgbm as lgb
import queue


def update_weight():                                                        
    """Create a callback that generates the weight for each query                                      
    Returns                                                                                            
    -------                                                                                            
    callback : function                                                                                
        The callback that prints the evaluation results every ``period`` iteration(s).                 
    """                    
    #Modifying the train set in the mbooster model
    def _callback(env):
        #set the weight if it is less than -2
        weight = []
        for result in env.risk_reward_list:
            if (result[0] == 'train'):
                flt = [x for x in result[2] if x < -2.0]
                print('Number of docs that is risky (<-2.0): ')
                print(len(flt))
                for idx, rr in enumerate(result[2]):
                    if (rr< -2.0):
                        weight += env.model.train_set.group[idx] * [1.0]
                    else:
                        weight += env.model.train_set.group[idx] * [0.8]
        weights = np.array(weight)
        env.model.train_set.weight = weights
    _callback.order = 10
    return _callback

class riskrewardUtil:

    bpath = "/research/remote/petabyte/users/robert/LightGBM/Experiments/"
    temppath = bpath + "resultsmslr10k/evalMetrics/"

    qidvalid = qrelsvalid = baselinename = baselineeval = ""
    qidtrain = qrelstrain = baselinetrainname = baselinetraineval = ""

    # parameterized constructor 
    def __init__(self, valqid, valqrels, baselinerun, baselineeval, 
                 qidtrain, qrelstrain, baselinetrainname, baselinetraineval, temppath): 
        self.qidvalid = valqid
        self.qrelsvalid = valqrels
        self.baselinename = baselinerun
        self.baselineeval = baselineeval
        self.qidtrain = qidtrain
        self.qrelstrain = qrelstrain
        self.baselinetrainname = baselinetrainname
        self.baselinetraineval = baselinetraineval
        self.temppath = temppath
        
    
    def evalScore(self, file, output, qrels, depth=5, bpath=bpath):
        bashCommand = "ls"
        script = "/research/remote/petabyte/users/robert/Utilities/grdeval/target/release/grdeval"
        countndcg = subprocess.run(script + ' -k %d %s %s | head -n -1 > %s' % (depth, qrels, file, output), shell=True, check=True, cwd = bpath)

    #Generate runfile based on the model (Predicting), getting the model, output, test_file, and qid
    #test_file = Features to predict (X file in svm light file) in numpy array
    #QID, give the qid in numpy array that is generated by load svm light file
    def predictgenerateRunFile(self, model, output, test_file, qid):
        bst = lgb.Booster(model_file=model)
        y_pred = bst.predict(test_file)

        #Script automation version
        result = pd.DataFrame(qid)

        #result['Pred'] = y_test << for generating qrel file
        result['Pred'] = y_pred

        #Append necessary column for trec eval
        result['model'] = 'LTR'
        result.insert(1, '2nd', 'Q0')

        #Get counts for each of the query ID and put them as the index
        qidCount = result.groupby(0)
        arr = pd.Series()
        for x in qidCount:
            #print(len(x[1]))
            arr = arr.append(pd.Series(range(1, len(x[1]) + 1)))
        arr = arr.reset_index(drop=True)
        result.insert(2, 'index', arr)

        #Create the Doc_ID for each of the query
        result.insert(2,'Doc_id', result[0].map(str) + "." + result['index'].map(str))

        #New Sorting version
        result = result.sort_values([0,'Pred'], ascending=[1,0]).reset_index(drop=True)

        #Reinsert the index
        result = result.drop(columns=['index'], axis=0)
        result.insert(3,'index',arr)

        result.to_csv(output, index=False, header=None, sep=' ')
        return result
        

    #Generating run file after we have the prediction
    def generateRunFile(self, pred, output, qid=qidvalid):
        #Script automation version
        result = pd.DataFrame(qid)

        #result['Pred'] = y_test << for generating qrel file
        result['Pred'] = pred

        #Append necessary column for trec eval
        result['model'] = 'LTR'
        result.insert(1, '2nd', 'Q0')

        #Get counts for each of the query ID and put them as the index
        qidCount = result.groupby(0)
        arr = pd.Series()
        for x in qidCount:
            #print(len(x[1]))
            arr = arr.append(pd.Series(range(1, len(x[1]) + 1)))
        arr = arr.reset_index(drop=True)
        result.insert(2, 'index', arr)

        #Create the Doc_ID for each of the query
        result.insert(2,'Doc_id', result[0].map(str) + "." + result['index'].map(str))

        #New Sorting version
        result = result.sort_values([0,'Pred'], ascending=[1,0]).reset_index(drop=True)

        #Reinsert the index
        result = result.drop(columns=['index'], axis=0)
        result.insert(3,'index',arr)

        result.to_csv(output, index=False, header=None, sep=' ')

    # f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool
    def trisk1(self, pred, train_data):

        # Globals
        alpha=1.0
        run_map = pred
        risk_reward = []
        baseline_map = []

        #Get bm25 features
        for row in range(0, train_data.data.shape[0]):
            baseline_map.append(train_data.data[row,109])

        runname = self.temppath + 'runfile'
        runeval = self.temppath + 'runeval'

        self.generateRunFile(run_map, runname, self.qidvalid)
        self.evalScore(runname, runeval, self.qrelsvalid)

        #check if baseline exist to generate only once

        checkfile = Path(self.baselinename)
        if(not checkfile.exists()):
            self.generateRunFile(baseline_map, self.baselinename, self.qidvalid)
            self.evalScore(self.baselinename, self.baselineeval, self.qrelsvalid)

        try:
            run_map = pd.read_csv(runeval)
            run_map = run_map['ndcg@5']
            baseline_map = pd.read_csv(self.baselineeval)
            baseline_map = baseline_map['ndcg@5']
        except:
            print("No Eval file found!")

        urisk = 0.0
        c = len(run_map)

        # Calculations from Dincer TRisk paper.
        def risk_reward_tradeoff_score(topic):
            r = run_map[topic]
            b = baseline_map[topic]

            if (r>b):
                return float((r - b))
            elif (r<b):
                return float((1 + alpha) * (r - b))
            else:
                return 0.000


        def sx():
            sum = 0.0
            for x in risk_reward:
                sum += ((x - urisk) ** 2)
            sum /= c
            return math.sqrt(sum)

        def parametric_standard_error_estimation():
            temp = sx()
            return (1.0 / math.sqrt(c)) * temp


        for index, topic in enumerate(run_map):
            val = risk_reward_tradeoff_score(index)
            risk_reward.append(val)

        # Calculate the mean of the risk reward scores. This is the URisk score.

        urisk = float(sum(risk_reward)) / float(len(risk_reward))

        # Calculate TRisk score
        se = parametric_standard_error_estimation()
        trisk = urisk / se

        return "Trisk alpha 1", float(trisk), True
    
    #Only use with the custom callback!
    #TODO: pass riskreward score to callback and generate the weight from there
    def riskrewardWeighted(self, pred, train_data):

        # Globals
        alpha=1.0
        run_map = pred
        risk_reward = []
        baseline_map = []
        score_per_topic = []
        

        #Get bm25 features
        for row in range(0, train_data.data.shape[0]):
            baseline_map.append(train_data.data[row,109])

        #Generate temp runfile and runeval
        runname = self.temppath + 'runfile'
        runeval = self.temppath + 'runeval'
        
        #It means this is a valid data because valid data is referenced to train data
        if (train_data.reference != None):
            qrels = self.qrelsvalid
            qid = self.qidvalid
            baselinename = self.baselinename
            baselineeval = self.baselineeval
        #It means this is a train data
        else:
            qrels = self.qrelstrain
            qid = self.qidtrain
            baselinename = self.baselinetrainname
            baselineeval = self.baselinetraineval
        
        #Generate each ndcg score evaluated
        self.generateRunFile(run_map, runname, qid)
        self.evalScore(runname, runeval, qrels)

        #check if baseline exist to generate only once
        checkfile = Path(baselinename)
        if(not checkfile.exists()):
            self.generateRunFile(baseline_map, baselinename, qid)
            self.evalScore(baselinename, baselineeval, qrels)

        try:
            run_map = pd.read_csv(runeval)
            run_map = run_map['ndcg@5']
            baseline_map = pd.read_csv(baselineeval)
            baseline_map = baseline_map['ndcg@5']
        except:
            print("No Eval file found!")
    
        urisk = 0.0
        c = len(run_map)

        # Calculations from Dincer TRisk paper.
        def risk_reward_tradeoff_score(topic):
            r = run_map[topic]
            b = baseline_map[topic]

            if (r>b):
                return float((r - b))
            elif (r<b):
                return float((1 + alpha) * (r - b))
            else:
                return 0.000

        def sx():
            sum = 0.0
            for x in risk_reward:
                sum += ((x - urisk) ** 2)
            sum /= c
            return math.sqrt(sum)

        def parametric_standard_error_estimation():
            temp = sx()
            return (1.0 / math.sqrt(c)) * temp

        for index, topic in enumerate(run_map):
            val = risk_reward_tradeoff_score(index)
            risk_reward.append(val)

        # Calculate the mean of the risk reward scores. This is the URisk score.
        urisk = float(sum(risk_reward)) / float(len(risk_reward))
        
        sxval = sx()
        for rr in risk_reward:
            score_per_topic.append(rr/sxval)
        
        
        #Read qrels, return index of query that is not evaluated on zeroindex
        temp = pd.read_csv(qrels, sep=' ', header=None)
        temp = temp.groupby(0)
        counter = 0
        #Contains index of query that is not evaluated
        zeroindex = []
        for idx, x in enumerate(temp):
            #Look for 0 value in the qrels
            if (not np.any(x[1][3])):
                counter+=1
                zeroindex.append(idx)
        
        #Filling the empty query to each of the index get from above
        a = np.arange(len(temp))
        #Fill each index of unlabelled query with 0 (unjudged)
        for x in zeroindex:
            a[x] = 0
        a[0] = 1
        counter = 0
        #Fill the rest with the score of the topic
        for y in score_per_topic:
            if (a[counter] != 0):
                a[counter] = y
                counter += 1
            else:
                while(a[counter] == 0):
                    counter+=1
                a[counter] = y
                counter+=1
        
        score_per_topic = a.tolist()
        
        # Calculate TRisk score
        se = parametric_standard_error_estimation()
        trisk = urisk / se
        
        #final score is the trisk value
        score_per_topic.append(trisk)
            
        return "Risk reward score: ", score_per_topic, True
